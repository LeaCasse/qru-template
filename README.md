# ğŸ“˜ QRU Template (1-Qubit Quantum Re-Uploading Unit)

A minimal, transparent, **hardware-ready QRU template** for PennyLane, with clean PyTorch integration, noise-simulation tools, and now **qBraid + IonQ backend support** (OpenQASM 2.0 export + execution on IonQ simulators).

This repo provides:

* a reusable 1-qubit **QRU block** implemented in PennyLane
* a configurable **TorchLayer wrapper** (normalization, scaling, clamping, batching)
* clean **regression/classification examples**
* NISQ **noise protocols**
* **hardware-ready pipeline**: train â†’ export QASM â†’ run via qBraid/IonQ

---

# ğŸ†• Whatâ€™s New (Dec 2025)

The repo now includes:

### âœ”ï¸ **`regression_sine_qbraid.py`**

A full demonstration of:

1. training a QRU(1q) on CPU (PennyLane + Torch)
2. exporting the trained circuit to **OpenQASM 2.0** with qBraid
3. running selected inference points on **IonQ simulated backends**:

   * `simulator` (ideal)
   * `simulator_aria1` (Aria-1 noise model)
   * `simulator_harmony` (Harmony noise model)

The workflow is **identical** to what would be required for running on a real IonQ QPU.

---

# ğŸ§  QRU Overview

Each QRU block applies:

```
RX(w[l, 0])
RY(w[l, 1] * x)   â† data re-uploading
RZ(w[l, 2])
```

For depth **L**, the circuit returns a single quantum feature:

[
\langle Z \rangle \in [-1,1].
]

The PyTorch wrapper adds:

* input normalization (`identity`, `zscore`, `minmax`)
* angle rescaling (`none`, `pi`, `2pi`)
* output mapping (`[-1,1] â†’ [a,b]`)
* periodic parameter constraints (wrap RX/RZ, clamp RY scale)
* batch loop (required with PennyLane â‰¤ 0.36)

---

# ğŸ“‚ Repository Structure

```
qru-template/
â”œâ”€ qru/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ qru_pennylane.py            # Core QRU + TorchLayer
â”‚  â””â”€ noise_protocol.md           # Noise simulation guide
â”‚
â”œâ”€ examples/
â”‚  â”œâ”€ regression_sine.py
â”‚  â”œâ”€ regression_sine_noisy.py
â”‚  â”œâ”€ regression_zscore_scaled.py
â”‚  â”œâ”€ classification_threshold.py
â”‚  â”œâ”€ classification_ce_fast.py
â”‚  â”œâ”€ verify_p001_g001_multi_seed.py
â”‚  â”œâ”€ regression_sine_qbraid.py   # NEW: qBraid/IonQ pipeline
â”‚
â”œâ”€ tests/
â”‚  â”œâ”€ test_shapes.py
â”‚  â””â”€ test_training_step.py
â”‚
â”œâ”€ results/
â”‚  â”œâ”€ noise_qru.csv
â”‚  â””â”€ noise_qru_p001_g001_seeds.csv
â”‚
â”œâ”€ README.md
â”œâ”€ LICENSE
â””â”€ requirements.txt
```

---

# âš™ï¸ Installation

### Core environment

```bash
pip install -r requirements.txt
```

### qBraid + IonQ support

```bash
pip install qbraid "qbraid[ionq]"
```

Set API keys:

```bash
set QBRAID_API_KEY=...
set IONQ_API_KEY=...
```

Edit your local `pyproject.toml` or install the repo as editable:

```bash
pip install -e .
```

---

# ğŸš€ Quick Start

### Verify installation

```bash
python - <<EOF
from qru import make_qru_torchlayer
print("QRU template OK:", callable(make_qru_torchlayer))
EOF
```

### Run tests

```bash
pytest -q    # expected: 2 passed
```

### CPU-only examples

```bash
python examples/regression_sine.py
python examples/classification_ce_fast.py
```

---

# ğŸ§ª Hardware-Ready Example (qBraid + IonQ)

### Train, export QASM, and run inference on IonQ simulators

```bash
python examples/regression_sine_qbraid.py --mode Q --device simulator --shots 200
```

Available devices on qBraid:

| ID                  | Description                       |
| ------------------- | --------------------------------- |
| `simulator`         | Ideal 29q simulator               |
| `simulator_aria1`   | Noisy sim (IonQ Aria-1 hardware)  |
| `simulator_harmony` | Noisy sim (IonQ Harmony hardware) |

This script:

1. trains a QRU(1q) (Torch + PL)
2. previews CPU predictions
3. exports circuit â†’ **OpenQASM 2.0**
4. submits QASM to qBraid IonQ runtime
5. displays backend counts

This is the recommended workflow for hardware-aligned experiments.

---

# ğŸ“Š Reference CPU Results (PennyLane 0.36)

| Example                       | Epochs | Metric | Result          |
| ----------------------------- | ------ | ------ | --------------- |
| `regression_sine.py`          | 100    | MSE â†“  | 0.45 â†’ 0.014    |
| `regression_zscore_scaled.py` | 100    | MSE â†“  | 0.022 â†’ 0.00023 |
| `classification_ce_fast.py`   | ~150   | Acc â†‘  | ~0.85â€“0.90      |

**Notes:**

* Normalizing inputs greatly improves stability.
* QRU(1q) can approximate nontrivial functions even at low depth.

---

# ğŸ”¬ Noise Experiments (NISQ-like)

Noise applied after each QRU block:

* `DepolarizingChannel(p)`
* `AmplitudeDamping(Î³)`
* `PhaseDamping(Î³)`

### Summary (single seed, p=Î³ grid)

| p \ Î³     | 0       | 0.001   | 0.01        |
| --------- | ------- | ------- | ----------- |
| **0**     | 0.02569 | 0.03254 | 0.09397     |
| **0.001** | 0.06018 | 0.01037 | 0.10646     |
| **0.01**  | 0.16276 | 0.03392 | **0.05776** |

### Multi-seed stability (p=Î³=0.01, 5 seeds)

MSEs = `0.03421, 0.01282, 0.03409, 0.12565, 0.08201`
â†’ mean Â± std = **0.0578 Â± 0.0408**

---

# ğŸ§± API Summary

```python
from qru import make_qru_torchlayer

model = make_qru_torchlayer(
    L=6,
    input_norm="zscore",
    input_stats={"mean": m, "std": s},
    input_angle_scale="pi",
    output_range=(0,1),
    ry_scale_max=1.0,
)

# IMPORTANT: call after opt.step()
model.constrain_()
```

---

# ğŸ§© Notes on Stability

* Normalize inputs when they vary strongly.
* Enforce periodicity for RX/RZ.
* Clamp RY scales to avoid exploding gradients.
* Batch loop is explicit (for PL â‰¤ 0.36).

---

# ğŸ“œ License

Apache 2.0 â€“ see `LICENSE`.

---
